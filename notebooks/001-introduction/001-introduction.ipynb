{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jetson Developer Kits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jetson family of modules all use the same NVIDIA CUDA-X™ software, and support cloud-native technologies like containerization and orchestration to build, deploy, and manage AI at the edge.\n",
    "\n",
    "<center><img src=\"./assets/kits.svg\"></center>\n",
    "\n",
    "\n",
    "[Details - Jetson Modules](https://developer.nvidia.com/embedded/jetson-modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDA CUDA-X\n",
    "\n",
    "**NVIDIA CUDA-X**, built on top of **CUDA**, is a collection of libraries, tools, and technologies that deliver dramatically higher performance than alternatives across multiple application domains—from artificial intelligence to high performance computing.\n",
    "\n",
    "<center><img src=\"./assets/cuda-x-ai-ecosystem-diagram-1ccw-d.png\"></center>\n",
    "\n",
    "\n",
    "[Details - NVIDIA CUDA-X](https://www.nvidia.com/en-us/technologies/cuda-x/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "\n",
    "**CUDA** (or Compute Unified Device Architecture) is a **parallel computing platform** and **application programming interface** (API) that allows software to use certain types of graphics processing unit (GPU) for general purpose processing\n",
    "\n",
    "<center><img src=\"./assets/cuda-accelerated-libraries.png\"></center>\n",
    "\n",
    "[Details CUDA Libraries](https://developer.nvidia.com/gpu-accelerated-libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA cuDNN\n",
    "\n",
    "The NVIDIA CUDA **Deep Neural Network library** (cuDNN) is a GPU-accelerated library of _primitives for deep neural networks_. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. cuDNN accelerates widely used deep learning frameworks, including Caffe2, Chainer, Keras, MATLAB, MxNet, PaddlePaddle, PyTorch, and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA TensorRT\n",
    "\n",
    "NVIDIA TensorRT is an SDK for **high-performance deep learning inference**. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications.\n",
    "\n",
    "<center><img src=\"./assets/nvidia-tensorrt-infographic.svg\"></center>\n",
    "\n",
    "- Tensorflow and keras: HDF5\n",
    "- Pytorch: .pt\n",
    "- Caffe2: .pb or .caffemodel\n",
    "- MXNet: .mxnet\n",
    "\n",
    "TensorRT provides an ONNX (Open Neural Network Exchange) parser to easily import ONNX models from popular frameworks into TensorRT\n",
    "\n",
    "[Details TensorRT](https://developer.nvidia.com/tensorrt)\n",
    "[Accelerating Inference Up to 6x Faster in PyTorch with Torch-TensorRT](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jetson Software\n",
    "\n",
    "<center><img src=\"./assets/image-for-developer-nvidia-com-embedded-develop-software.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jetson Hardware\n",
    "\n",
    "<center><img src=\"./assets/jetson-nano-dev-kit-top-r6-HR-B01.png\"></center>\n",
    "\n",
    "\n",
    "1. microSD card slot for main storage\n",
    "2. 40-pin expansion header\n",
    "3. Micro-USB port for 5V power input, or for Device Mode\n",
    "4. Gigabit Ethernet port\n",
    "5. USB 3.0 ports (x4)\n",
    "6. HDMI output port\n",
    "7. DisplayPort connector\n",
    "8. DC Barrel jack for 5V power input\n",
    "9. MIPI CSI-2 camera connectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
